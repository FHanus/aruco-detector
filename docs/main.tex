\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}

\input{template}

\begin{document}

\title{Deep Learning Methods for ArUco Marker Detection and Classification Under Challenging Distortions}

\author{
\IEEEauthorblockN{
    Filip Hanu≈°,
}
\IEEEauthorblockA{
    School of Engineering, College of Art, Technology and Environment,\\
    University of the West of England, Bristol, UK\\
    Email: filip2.hanus@uwe.ac.uk}
}

\maketitle
\begin{abstract}

This project investigates deep learning methodologies for detecting and classifying ArUco markers in the presence of severe distortions such as blur,
noise, and rotations. The project addresses two primary challenges: marker classification under varying distortion levels and accurate localisation in
affected images. For classification, several architectures were evaluated, with GoogLeNet achieving 100\% accuracy on weakly distorted markers and
97.48\% under severe distortions. For detection, RetinaNet-ResNet50 showed impressive performance at 89.00\% IoU in standard conditions and 85.64\%
under severe distortions. Using synthetic datasets and transfer learning, the results confirm that pre-trained models outperform custom architectures.
While effective in real-world conditions, further improvements are feasible with enhanced computational resources.

\end{abstract}

\begin{IEEEkeywords}
ArUco Markers, Deep Learning, Computer Vision, Object Detection, Image Classification, Fiducial Markers
\end{IEEEkeywords}

\section{Introduction}

ArUco markers (Figure \ref{fig:aruco_marker_basic}) are square visual codes designed for efficient detection and identification
using computer vision algorithms. They are widely used in applications such as robotics, virtual reality, and industrial automation
for positioning, tracking, and navigation tasks. Examples of these markers in use are shown in Figure \ref{fig:aruco_markers}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.1\textwidth]{images/aruco-marker-1.png}
    \caption{Basic ArUco marker example}
    \label{fig:aruco_marker_basic}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/aruco-example-1.png}
        \caption{Robot localisation (\cite{AI-Assisted-Drone-Localization})}
        \label{fig:aruco1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/aruco-example-2.png}
        \caption{Drone localisation (\cite{Nakajima2024AboutAE})}
        \label{fig:aruco2}
    \end{subfigure}
    \caption{Examples of ArUco markers in different conditions}
    \label{fig:aruco_markers}
\end{figure}

However, as noted by \textcite{FiducialMarkerNoisy} and \textcite{ROMERORAMIREZ2021104094}, traditional detection methods perform well
under controlled conditions but struggle with real-world challenges such as rotation, blur, and noise. This project investigates deep
learning approaches to address these limitations. The key objectives are:

\begin{enumerate}
  \item Classifying ArUco markers under challenging conditions (blur, noise, and rotation).
  \item Detecting and tracking marker positions in noisy or distorted environments.
\end{enumerate}

\section{Classification}

\subsection{Classification Methodology}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file3-classification-1.png}
      \caption{Class 0 (out of 100) marker example 1}
      \label{fig:class_ex1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file3-classification-2.png}
      \caption{Class 0 (out of 100) marker example 2}
      \label{fig:class_ex2}
  \end{subfigure}
  
  \vspace{0.5cm}
  
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file3-classification-3.png}
      \caption{Class 1 (out of 100) marker example 1}
      \label{fig:class_ex3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file3-classification-4.png}
      \caption{Class 1 (out of 100) marker example 2}
      \label{fig:class_ex4}
  \end{subfigure}
  \caption{Examples of ArUco marker classification under different conditions (from the provided dataset)}
  \label{fig:classification_examples}
\end{figure}

The classification dataset examples are visible in Figure \ref{fig:classification_examples}. The dataset contains
100 variations/classes of the ArUco marker. Developing a classification model was divided into the following steps:

\begin{enumerate}
  \item Create a data visualisation tool for datasets and results.
  \item Develop a data augmentation pipeline to expand the dataset.
  \item Prepare and implement suitable classification architectures.
  \item Design an experiment to evaluate architectures and parameters.
  \item Train and test the chosen classification model.
\end{enumerate}

\subsubsection{Data Preparation}

Firstly, a dataset browser was created to understand the data. This tool provided insight into image characteristics,
pixel distribution, and basic dataset statistics. It was also designed to visualise both datasets and results, as
demonstrated in Figure \ref{fig:data_browser_1}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{images/aruco-dataset-browser-1.png}
  \caption{Dataset browser used with the File2 dataset}
  \label{fig:data_browser_1}
\end{figure}

A data augmentation pipeline was developed to generate a larger, more robust dataset. Through trial and error, augmentation
parameters were optimised to mimic the distortions observed in File3. The pipeline expanded the original dataset (100 images
in File1, each representing a different ArUco marker) to 1,000 images per class at a resolution of 512x512 pixels. Augmentations, 
including rotation, blurring, and noise, were configured via a central configuration file, allowing for flexibility without 
needing to modify the codebase. The pipeline workflow is summarised below:

\begin{figure}[H]
  \begin{algorithm}[H]
  \caption{Data Augmentation Pipeline}
  \begin{algorithmic}[1]
  \STATE List all of the images in the File1 dataset
  \WHILE{size of augmentations list $<$ target dataset size}
      \STATE Select random rotation, between 0 and 360 degrees
      \STATE Select random scaling, between 50\% and 100\% of original size
      \STATE Select random blur, between 0 and 14 sigma
      \STATE Select random noise, between 0\% and 10\% of pixels
      \STATE Append the set of augmentations to the list
    \ENDWHILE
  \FOR{each image in File1}
      \STATE Create folder named after the image's number (0-99)
      \STATE Read the raw image
      \FOR{each set in the list of augmentations}
        \STATE Apply the augmentations to the image
        \STATE Save the augmented image to the new folder
      \ENDFOR
    \ENDFOR
  \end{algorithmic}
  \end{algorithm}
  \caption{Classification training workflow}
  \end{figure}

\subsubsection{Network Architectures}

Several classification architectures were implemented using PyTorch, starting with a basic convolutional neural network (CNN)
to establish baseline performance. Following this, pre-trained and non-pre-trained variants of AlexNet were tested. ResNet18
and GoogLeNet were also evaluated, leveraging PyTorch documentation and guidelines \textcite{pytorch_models}. The codebase was
modular, allowing seamless integration of new models into experiments.

The following architectures were selected for initial experimentation:

\begin{itemize}
    \item \textbf{MinimalCNN}: A lightweight custom architecture with:
    \begin{itemize}
        \item Three convolutional blocks (32$\rightarrow$64$\rightarrow$128 features)
        \item Batch normalisation and dropout for regularisation
        \item Global average pooling for dimension reduction
    \end{itemize}
    
    \item \textbf{AlexNet Variants}:
    \begin{itemize}
        \item Clean implementation without pre-training
        \item Version with pre-trained features
    \end{itemize}
    
    \item \textbf{ResNet18}: Pre-trained implementation
    \item \textbf{GoogLeNet}: Pre-trained implementation
\end{itemize}

\subsubsection{Training Configuration}

The experiments tested combinations of the above architectures with varied parameters:

\begin{itemize}
  \item \textbf{Datasets}: File3 (100 images per class, significant distortions).
  \item \textbf{Models}: MinimalCNN, AlexNet (clean), AlexNet, ResNet18, GoogLeNet.
  \item \textbf{Batch Sizes}: 32, 64.
  \item \textbf{Training Transformations}: None, random rotations, and combinations of rotations, blur, and noise.
\end{itemize}

This resulted in 30 parameter/model combinations, trained over nearly 8 hours on a single GPU. Customisation required
minimal effort, achieved via a central configuration file. Adjustments could be made to datasets, batch sizes,
transformations, and models, although larger batch sizes increased memory demands.

\begin{itemize}
    \item \textbf{Loss Function}: Cross-entropy
    \item \textbf{Optimizer}: Adam with learning rate 3e-4
    \item \textbf{Training Duration}: 50 epochs with early stopping at 96\% training accuracy 
\end{itemize}

Once training was complete, results were plotted and analysed. Full results are detailed in the appendices and results directory.
The final classification model was trained on the custom dataset with the following optimised parameters:

\begin{itemize}
  \item \textbf{Datasets}: Custom dataset (1000 images per class)
  \item \textbf{Model}: GoogLeNet
  \item \textbf{Batch Sizes}: 32
  \item \textbf{Additional Random Training Transformations}: None
\end{itemize}

\subsubsection{Method Implementation}

The experiment implementation followed this algorithmic workflow:

\begin{figure}[H]
\begin{algorithm}[H]
\caption{Classification Experiment Pipeline}
\begin{algorithmic}[1]
\STATE Load configuration for dataset, model, batch size, and transformations
\FOR{each combination in configurations}
    \STATE Load dataset
    \STATE Setup results directory
    \STATE Initialise model and transforms
    \STATE Create data loaders
    \STATE Initialise loss function, optimiser
    \STATE Train and validate model for number of epochs
    \STATE Plot the training progress
    \STATE Run evaluation on test set
    \STATE Plot classification metrics
    \STATE Save all results
\ENDFOR
\end{algorithmic}
\end{algorithm}
\caption{Classification experiment workflow}
\end{figure}

The model training loop is shown below:

\begin{figure}[H]
\begin{algorithm}[H]
\caption{Classification Training and Validation Pipeline}
\begin{algorithmic}[1]
\FOR{each epoch in total number of epochs}
    \STATE Set model to train mode
    \FOR{each batch in train loader}
        \STATE Reset gradients
        \STATE Forward pass through network
        \STATE Calculate cross-entropy loss
        \STATE Backpropagate and update weights
        \STATE Add loss and accuracy to totals
    \ENDFOR
    \STATE Calculate average training loss and accuracy
    \STATE Set model to evaluation mode
    \FOR{each batch in validation loader}
        \STATE Forward pass through network
        \STATE Calculate cross-entropy loss
        \STATE Add loss and accuracy to totals
    \ENDFOR
    \STATE Calculate average validation loss and accuracy
    \STATE Save model if the best validation accuracy was achieved
    \STATE Check for early stopping criteria
\ENDFOR
\end{algorithmic}
\end{algorithm}
\caption{Classification training workflow}
\end{figure}

After training, the model was tested using the original datasets (File2 and File3) to evaluate its
generalisation capabilities without seeing the originally provided data. Test-only data loaders were
created for this purpose, and the model performance on these datasets was separately recorded.

\subsection{Classification Results}

The classification experiments were conducted in two phases. Initially, multiple architectures
and configurations were tested on the File3 dataset to determine the optimal approach. Subsequently, the best
performing configuration was trained on a custom dataset and evaluated against both File2 and File3.

\subsubsection{Initial Experiments}

The experiments encompassed various architectures with different batch sizes and data augmentation strategies. Each
configuration was tested on 1000 samples from File3, with the following results:

\begin{itemize}
    \item \textbf{MinimalCNN}:
        \begin{itemize}
            \item Batch size 32, full (training time) augmentation: 5.20\% accuracy (F1: 3.62\%)
        \end{itemize}

    \item \textbf{AlexNet (Clean Implementation)}:
        \begin{itemize}
            \item Batch size 64, rotation (training time) augmentation: 98.60\% accuracy (F1: 98.51\%)
        \end{itemize}
    
    \item \textbf{AlexNet (Pre-trained)}:
        \begin{itemize}
            \item Batch size 64, full (training time) augmentation: 73.70\% accuracy (F1: 72.15\%)
        \end{itemize}
    
    \item \textbf{ResNet18}:
        \begin{itemize}
            \item Batch size 32, no (training time) augmentation: 100.00\% accuracy (F1: 100.00\%)
        \end{itemize}
    
    \item \textbf{GoogLeNet}:
        \begin{itemize}
            \item Batch size 32, no (training time) augmentation: 99.90\% accuracy (F1: 99.87\%)
        \end{itemize}
\end{itemize}

The experiment results are shown in the figure \ref{fig:training_progress}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/training_progress_MinimalCNN.png}
      \caption{MinimalCNN training progress}
      \label{fig:progress_minimalcnn}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/training_progress_AlexNet_clean.png}
      \caption{AlexNet (clean) training progress}
      \label{fig:progress_alexnet_clean}
    \end{subfigure}    
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/training_progress_AlexNet.png}
      \caption{AlexNet (pre-trained) training progress}
      \label{fig:progress_alexnet}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/training_progress_ResNet.png}
        \caption{ResNet18 training progress}
        \label{fig:progress_resnet}
    \end{subfigure}
    \caption{Training progress for different model architectures showing training and validation accuracy over epochs (orange plot is validation accuracy,
    blue is training accuracy)}
    \label{fig:training_progress}
\end{figure}

All of the training progress plots show the training progress of the models with 32 batch size and no data augmentation for comparability.
The results of the minimal CNN are as expected, although the model was indeed training and given many more epochs, it would likely achieve better results.  

As expected, the AlexNet with no pre-trained weights performed very well, but converged much slower than it's pre-trained counterpart.
ResNet18 and GoogLeNet preformed near perfectly, noticebly better without additional training transforms than with them. Only the ResNet's plot
is shown as their are nearly the same. These two models triggered early stopping after reaching 99\% validation accuracy.

For further insights, training logs and detailed analysis, additional plots, results, and data can be found in the Appendices.

\subsubsection{Final Model Performance}

Based on the previous findings, GoogLeNet was selected for the final model due to its consistent performance across different parameter settings.
The model was trained on FileCustom1, a synthetic dataset created to approximate the conditions in Files 2 and 3. This means that the training
data increased from 9,000 samples to 95,000 samples.

The final evaluation yielded these results:

\begin{itemize}
    \item \textbf{Training Evaluation} (2,500 samples):
        \begin{itemize}
            \item Accuracy: 100.00\%
            \item Precision: 100.00\%
            \item Recall: 100.00\%
            \item F1-score: 100.00\%
        \end{itemize}
    \item \textbf{File2 Evaluation} (10,000 samples):
        \begin{itemize}
            \item Accuracy: 100.00\%
            \item Precision: 100.00\%
            \item Recall: 100.00\%
            \item F1-score: 100.00\%
        \end{itemize}
    \item \textbf{File3 Evaluation} (10,000 samples):
        \begin{itemize}
            \item Accuracy: 97.48\%
            \item Precision: 97.68\%
            \item Recall: 97.48\%
            \item F1-score: 97.51\%
        \end{itemize}
\end{itemize}

These results demonstrate the model's near-perfect performance (as shown in figure \ref{fig:final_analysis}) across different distortion conditions.
The perfect scores on File2 indicate excellent handling of weak distortions, while the very good performance on File3
(misclassifying only 252 out of 10,000 samples) shows good resilience to more challenging conditions. Based on the accuracy, precision, and recall
plots per class, we can clearly see that the model fails to identify marker 33 the most as that's where it's precision is the lowest,
whilst markers 14 and 39 are the most frequently missed ones due to the lowest recall. This can be better visualised in the confusion matrix (this can be seen in
the appendices).

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/File3_final_analysis_accuracy.png}
        \caption{Per-class accuracy analysis}
        \label{fig:final_accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/File3_final_analysis_precision.png}
        \caption{Per-class precision analysis}
        \label{fig:final_precision}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/File3_final_analysis_recall.png}
        \caption{Per-class recall analysis}
        \label{fig:final_recall}
    \end{subfigure}
    \caption{Final model performance analysis on File3 dataset showing per-class metrics}
    \label{fig:final_analysis}
\end{figure}

\section{Detection}

\subsection{Detection Methodology}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file4-detection-1.png}
      \caption{Marker in office placement example 1}
      \label{fig:det_ex_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file4-detection-2.png}
      \caption{Marker in office placement example 2}
      \label{fig:det_ex_2}
  \end{subfigure}
  
  \vspace{0.5cm}
  
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file4-detection-3.png}
      \caption{Marker in office placement example 3}
      \label{fig:det_ex_3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.2\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/aruco-file4-detection-4.png}
      \caption{Marker in office placement example 4}
      \label{fig:det_ex_4}
  \end{subfigure}
  \caption{Examples of ArUco marker placements (from the provided dataset)}
  \label{fig:detection_examples}
\end{figure}

The detection dataset examples are visible in figure \ref{fig:detection_examples}.
There are 100 examples of the ArUco marker placement and a CSV file with the position of the marker in each one of them.

The detection problem was approached in a similar way to classification, with the following key steps:

\begin{enumerate}
  \item Adapt and prepare the data plotter for detection datasets and results
  \item Develop data augmentation script for office images with ArUco markers
  \item Prepare detection network architectures
  \item Design experiment to compare detection architectures and settings
  \item Train and test the final detection model
\end{enumerate}

\subsubsection{Data Preparation}

Firstly, the dataset browser was asdapted to handle detection data viewing (and results).
It shows the image, it's bounding box (ground truth and prediction if available), pixel distribution, and basic dataset statistics.
This can be seen in Figure \ref{fig:data_browser_2}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{images/aruco-dataset-browser-2.png}
  \caption{Dataset browser used with the File4 dataset}
  \label{fig:data_browser_2}
\end{figure}

After analysing the data, the data augmentation script was developed. It was used to create a larger dataset for training, as the provided one
for this task was insufficient compared to the classification task. The data browser was used to identify the optimal way to approach this.

The methodology of the determined data augmentation pipeline is shown below:

\begin{figure}[H]
\begin{algorithm}[H]
\caption{Detection Data Preparation Pipeline}
\begin{algorithmic}[1]
\STATE Load office images and ArUco tags
\FOR{each office image}
    \STATE Get random 1000x1000 crop from original image
    \STATE Resize cropped image to 224x224
    \FOR{each ArUco class}
        \STATE Select random tag from custom dataset 1
        \STATE Resize tag to 34x34
        \STATE Calculate valid random position
        \STATE Place tag at position
        \STATE Save combined image
        \STATE Store bounding box coordinates
    \ENDFOR
\ENDFOR
\STATE Save bounding box annotations to CSV
\end{algorithmic}
\end{algorithm}
\caption{Detection data preparation workflow}
\end{figure}

\subsubsection{Network Architectures}

Multiple pre-trained architectures were implemented from the PyTorch library. Many of these were quite simple to implement and
worked out of the box, such as RetinaNet and MobileNet, and many (for example the ones from the lecture materials) proved troublesome to implement sucesfully.
Final models were based on \textcite{pytorch_retinanet}, \textcite{pytorch_fasterrcnn}, and \textcite{pytorch_mobilenet_rcnn} and were
chosen for their performance during development, and ease of implementation:

\begin{itemize}
    \item \textbf{RetinaNet with ResNet50 backbone}: $\sim$40M parameters
    \begin{itemize}
        \item Pre-trained on ImageNet dataset for general object recognition (\cite{pytorch_retinanet})
    \end{itemize}
    
    \item \textbf{Faster R-CNN with ResNet50 backbone}: $\sim$38M parameters
    \begin{itemize}
        \item Pre-trained on ImageNet dataset for general object recognition
        \item Two-step detection: first finds regions of interest, then classifies them (\cite{pytorch_fasterrcnn})
    \end{itemize}
    
    \item \textbf{MobileNetV3-Large with FPN}: $\sim$20M parameters
    \begin{itemize}
        \item Pre-trained on ImageNet dataset for general object recognition
        \item Originally optimised for mobile/edge devices with fewer parameters (\cite{pytorch_mobilenet_rcnn})
    \end{itemize}
\end{itemize}

\subsubsection{Training Configuration}

Similarly to classification, the detection training pipeline included:

\begin{itemize}
  \item \textbf{Datasets}: File5 (provided dataset with 100 images of the office with tags with strong distortions to test the detection models)
  \item \textbf{Models}: RetinaNet-ResNet50, FasterRCNN-ResNet50, MobileNetV3-Large-FPN
  \item \textbf{Batch Sizes}: 4
  \item \textbf{Additional Random Training Transformations}: None, this is not to distort the office images, since the tags are already distorted
\end{itemize}

These experiment parameters form only 3 different networks and sets of results. 
With a single change in the configuration file, the entire experiment can be changed, the custom dataset can be used (raapidly increasing time per epoch), different
batch sizes can be used (increasing memory usage beyond the used GPU's capability), and different models can be used.

Additional settings and parameters purely for the purpose of limiting the time to run the experiment were used:

\begin{itemize}
    \item \textbf{Optimizer}: Adam with learning rate 3e-4
    \item \textbf{Training Duration}: 100 epochs with early stopping at 98\% training accuracy 
\end{itemize}

After the experiments were completed, the results were plotted and analysed. The results are shown in the following section, and full results are available in the results directory
and the appendices.

The final model was trained on the custom dataset, with the following parameters and settings derived based on the best performing combination of the initial experiments:

\begin{itemize}
  \item Batch size of 8 (GPU memory constrained)
  \item 50 training epochs
  \item Early stopping at 99.6\% IoU threshold
  \item Separate evaluation on File4 and File5
\end{itemize}

The implementation does not differ from the classification one. To calculate the progress of the training, the Intersection over Union (IoU) metric was used. This 
seemed to be the most appropriate metric for this task, as it measures the overlap between the predicted and true bounding boxes, which in the
case of this project is only one box and only one class per image. The metrics tracked during evaluation are the mean IoU and mean Mean Absolute Error (MAE),
which seem to be the most appropriate metrics for this task.

\subsection{Detection Results}

The detection experiments were conducted in two phases, similar to classification. First, multiple architectures were tested on File5
dataset to determine the optimal approach. Then, the best performing configuration was trained on a custom dataset and
evaluated against both File4 and File5.

\subsubsection{Initial Experiments}

The experiments encompassed the selected architectures with batch size of 4 (due to GPU memory constraints). Each
configuration was tested on 1000 samples from File5, with the following results:

\begin{itemize}
  \item \textbf{FasterRCNN-ResNet50}:
    \begin{itemize}
        \item Batch size 4: Mean IoU: 97.80\%
    \end{itemize}
    
  \item \textbf{MobileNetV3-Large-FPN}:
    \begin{itemize}
        \item Batch size 4: Mean IoU: 95.93\% 
    \end{itemize}

  \item \textbf{RetinaNet-ResNet50}:
    \begin{itemize}
        \item Batch size 4: Mean IoU: 98.05\%
    \end{itemize}
\end{itemize}

The experiment results are shown in the figure \ref{fig:training_progress_detection}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/File5_FasterRCNN_training_progress.png}
      \caption{FasterRCNN training progress}
      \label{fig:progress_fasterrcnn}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/File5_MobileNet_training_progress copy.png}
      \caption{MobileNet (pre-trained) training progress}
      \label{fig:progress_mobilenet}
    \end{subfigure}    
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/File5_RetinaNet_training_progress.png}
      \caption{RetinaNet (pre-trained) training progress}
      \label{fig:progress_retinanet}
    \end{subfigure}
    \caption{Training progress for different model architectures showing training loss and IoU over epochs}
    \label{fig:training_progress_detection}
\end{figure}

It can be seen that the RetinaNet model performed the best, converged the fastest, with the highest IoU and lowest loss.

\subsubsection{Final Model Performance}

Based on the previous findings, RetinaNet with ResNet50 backbone was selected for the final model due to its superior performance
across different metrics. The model was trained on FileCustom2, a synthetic dataset created to approximate the conditions
in Files 4 and 5. The training dataset was selected to consist of 10,000 images, but it can be changed with a single change in the configuration file.
This was done to save time and resources, as the training dataset was already large enough to train the model
and both the augmentation and each epoch of training took a long time to complete.

The final evaluation yielded these results:

\begin{itemize}
    \item \textbf{Training Evaluation} (1,000 testing samples):
        \begin{itemize}
            \item Mean IoU: 99.70\%
        \end{itemize}
    \item \textbf{File4 Evaluation} (100 of the provided samples):
        \begin{itemize}
            \item Mean IoU: 89.00\%
        \end{itemize}
    \item \textbf{File5 Evaluation} (100 of the provided samples):
        \begin{itemize}
            \item Mean IoU: 85.64\%
        \end{itemize}
\end{itemize}

These results demonstrate the model's strong performance across different distortion conditions.
The perfect detection rate (IoUs over 50\%) on File4 indicates excellent handling of weak distortions,
while the very good performance on File5 (failing to detect only 3 out of 100 markers) shows good resilience
to more challenging conditions. As seen in figure \ref{fig:final_detection_examples}, some of the markers were not detected, which is a problem that
needs to be addressed in the future.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/result_vis_1.png}
    \caption{Example 1}
    \label{fig:det_res_ex1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/result_vis_2.png}
    \caption{Example 2}
    \label{fig:det_res_ex2}
  \end{subfigure}    
  \vspace{0.5cm}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/result_vis_4.png}
    \caption{Example 3}
    \label{fig:det_res_ex3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/result_vis_5.png}
      \caption{Example 4}
      \label{fig:det_res_ex4}
  \end{subfigure}
  \caption{Final detection model prediction examples}
  \label{fig:final_detection_examples}
\end{figure}

\section{Conclusion}

This work explored deep learning approaches for ArUco marker detection and classification under challenging conditions.
The investigation yielded several key findings:

\begin{itemize}
    \item \textbf{Classification Performance}: GoogLeNet achieved near-perfect accuracy (100\%) on weakly distorted markers
    and maintained robust performance (97.48\%) even under severe distortions, demonstrating the effectiveness of transfer learning for this task.
    The custom model performed better during longer training, this was proved in the commit d9fd8ad.
    
    \item \textbf{Detection Robustness}: RetinaNet with ResNet50 backbone showed excellent performance in locating markers,
    achieving 89.00\% IoU on basic office scenarios and maintaining 85.64\% IoU under challenging conditions,
    with occasional (0,0) position predictions pointing to the need for longer training.
    
    \item \textbf{Architecture Insights}: Pre-trained models consistently outperformed custom architectures.
    
    \item \textbf{Technical Limitations}: GPU memory constraints limited batch sizes and training duration, with detection model epochs
    taking 20 minutes each. With more compute, the batch size should be increased and the dataset size expanded using the configuration file
    in order to yield better detection results.

    \item \textbf{Augmentation Mistake}: Additional data augmentation during training on already pre-augmented images proved counterproductive.
\end{itemize}

Future improvements that could generate very interesting results could include:
\begin{itemize}
    \item Expanding synthetic dataset generation to include perspective transformations, partial occlusion, and other distortions
    \item Implementing larger batch sizes and extended training
    \item Exploring real-world data collection to validate synthetic training results
    \item Testing more pre-trained detection models with sufficient computational resources
\end{itemize}

\printbibsection

\appendices

\renewcommand{\thesection}{\Alph{section}}

\section{GitHub Repository}

The GitHub repository for this project can be found at the following link: \url{https://github.com/FHanus/aruco-detector}.

The repository contains the following key components:

\begin{enumerate}
    \item \textbf{Classification, per trained model:}
    \begin{itemize}
        \item Per class accuracy plot
        \item Per class confusion matrix
        \item Per class precision plot
        \item Per class recall plot
        \item Training progress plot
        \item CSV with evaluation predictions and data
    \end{itemize}

    \item \textbf{Classification, per trained model:}
    \begin{itemize}
        \item IoU distribution plot
        \item Training progress plot
        \item CSV with evaluation predictions and data
    \end{itemize}

    \item \textbf{Datasets:}
    \begin{itemize}
        \item Original datasets (File1-File5)
        \item Custom synthetic datasets (Custom1-Custom2)
    \end{itemize}

    \item \textbf{Documentation:}
    \begin{itemize}
        \item This LaTeX paper
        \item README.md
    \end{itemize}

    \item \textbf{Infrastructure:}
    \begin{itemize}
        \item All of the code
    \end{itemize}
\end{enumerate}

This repository represents a complete research project on ArUco marker detection and classification, including code implementation, experimental results, and detailed documentation.

\end{document}