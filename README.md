# ArUco Tag Detection and Classification

This project implements deep learning techniques for robust ArUco marker detection and classification, working even in tricky situations like when markers are rotated, blurred or noisy.

## Project Structure

```
.
├── data/                                      # Dataset directory
│   ├── File1/                                 
│   │   └── arucoRaw/                          # Clean ArUco images (512x512)
│   ├── File2/                                 
│   │   └── arucoBasic/                        # Weakly distorted ArUco images
│   ├── File3/                                 
│   │   └── arucoChallenging/                  # Strongly distorted ArUco images
│   ├── File4/                                
│   │   └── combinedPicsBasic/                 # Office with randomly placed ArUco images
│   ├── File5/                                 
│   │   └── combinedPicsChallenging/           # Office with randomly placed ArUco images
│   ├── File6/                                
│   │   └── officePics/                        # Original office images
├── scripts/
│   ├── main.py                                # Primary execution script
│   ├── src/                                  
│   │   ├── data_exploration.py                # Dataset/results analysis and visualisation
│   │   ├── dataset_augmentation.py            # ArUco augmentation pipeline
│   │   ├── combine_office_tags.py             # Office and ArUco images combination
│   │   ├── run_classification_experiments.py  # Classification testing
│   │   ├── run_detection_experiments.py       # Detection testing
│   │   ├── train_classifier.py                # Classification model training
│   │   └── train_detector.py                  # Detection model training
│   └── utils/                                 
│       ├── architectures.py                   # Model architectures
│       ├── data_loaders.py                    # Dataset loading utilities
│       ├── model_analysis.py                  # Performance analysis tools
│       └── training_utils.py                  # Training helper functions
└── requirements.txt                           
```

## What Each Script Does

### Main Scripts

#### data_exploration.py
A visualisation tool that lets you look through the datasets and results. You can:
- See stats about the dataset
- Look at where the bounding boxes are, or their predictions (if available)

```bash
python3 data_exploration.py --dataset 
python3 data_exploration.py --detection
```

#### dataset_augmentation.py
Outputs ArUco tag variations by:
- Rotating (0-360°)
- Blurring (σ ranging from 0-14)
- Adding noise (up to 10% of pixels)
- Resizing (50-100% of original)
- The generated dataset is meant for 'train_classifier.py'

#### combine_office_tags.py
Combines ArUco tags with office images:
- Processes office images to 1000x1000 crop
- Resizes to 224x224 final size
- Places 34x34 ArUco tags (from the dataset generated by 'dataset_augmentation.py') at random positions
- Generates bounding box annotations CSV
- The generated dataset is meant for 'train_detector.py'

#### run_classification_experiments.py
Classification model experimentation framework. The only limitation in terms of the amout of tested parameters is the computation time of going through all the iterations:
- Tests multiple architectures (custom tiny CNN, AlexNet without weights, pretrained AlexNet, pretrained ResNet18, pretrained GoogLeNet)
- Various batch sizes (64, 128)
- Different data augmentation strategies 
- Saves results for each configuration

#### run_detection_experiments.py
Detection model experimentation framework. The only limitation in terms of the amout of tested parameters is the computation time of going through all the iterations:
- Tests multiple architectures:
  * RetinaNet-ResNet50
  * FasterRCNN-ResNet50
  * MobileNetV3-Large-FPN
- Tiny batch size because I couldn't fit more
- Evaluation metrics
- Results can be visualised using the 'data_exploration.py' script

#### train_classifier.py
Instead of testing the best parameters combination, parameters are selected and the model is trained on the custom augmented (as large as desired) dataset.
- Saves results just like the experiment script

#### train_detector.py
Instead of testing the best parameters combination, parameters are selected and the model is trained on the custom augmented (as large as desired) dataset.
- Saves results just like the experiment script

### Utility Scripts

#### architectures.py
Model architecture implementations:
- MinimalCNN (custom)
- Pretrained (+ clean) AlexNet 
- Pretrained ResNet18
- Pretrained GoogLeNet
- Detection models (RetinaNet, FasterRCNN, MobileNetV3) - only those that were easily implementable from PyTorch, and worked with no issues out of the box. 

#### data_loaders.py
Dataset handling utilities:
- DataLoaders for all of the training scripts
- Train/val/test split functionality

#### model_analysis.py
Performance analysis tools for both classification and detection:
- Training statistics
- Model performance statistics

#### training_utils.py
Training support functions:
- Training loops
- Validation checks
- Early stopping
- Result logging

## Requirements

- Built with Python 3.10.12
- Matplotlib : Plotting, visualisations
- NumPy : Numerical operations on arrays and matrices
- OpenCV-Python : Image processing functions
- Pandas : DataFrame object handler (CSV)
- Scikit-learn : Tools for model evaluation
- Seaborn : Pretty heatmaps (confusion matrices)
- PyTorch (torch) : ML core

## Getting Started

1. First off, set up your virtual environment:
```bash
python -m venv aruco-venv
source aruco-venv/bin/activate  # Linux/Mac
# or
.\aruco-venv\Scripts\activate   # Windows
```

2. Sort out the dependencies:
```bash
pip install -r requirements.txt
```

## How to Use It

The system's got two main bits:

### Classification

Learns ArUco patterns from Files 2 and 3 (the ones with weak and strong distortion), does this with multiple networks and parameters to compare them. Run:

```bash
python scripts/src/run_classification_experiments.py
```

Results of this are stored as "results/STP1.." 

Targeted script for only training one network with the same purpose can be run with:

```bash
python scripts/src/train_classifier.py
```

Results of this are stored as "results/STP2.." 

### Detection

Locates ArUco patterns (both with weak and strong distortion) in office environment images from Files 4 and 5, does this with multiple networks and parameters to compare them.  Run:

```bash
python scripts/src/run_detection_experiments.py
```

Results of this are stored as "results/STP3.." 

Targeted script for only training one network with the same purpose can be run with:

```bash
python scripts/src/train_detector.py
```

Results of this are stored as "results/STP4.." 

### Full Pipeline

To execute the complete pipeline including data preparation, training and evaluation:

```bash
python scripts/main.py
```

This runs:
1. Dataset augmentation
2. Office tag combination
3. Classification experiments (results are saved as step 1)
4. Classifier training (results are saved as step 2)
5. Detection experiments (results are saved as step 3)
6. Detector training (results are saved as step 4)

## Technical Details

### Classification Network

- MinimalCNN: A simple architecture 
  * 3 conv blocks (32->64->128 features)
  * Batch normalisation and dropout
  * Global average pooling
- AlexNet variants:
  * One without any pre-training
  * Another with frozen features from pre-training
- ResNet18 (with weights)
- GoogLeNet (with weights)

Final model:
- ResNet18 architecture
- Training configuration:
  * Adam optimizer (β1=0.9, β2=0.999)
  * CrossEntropy loss
  * Learning rate: 3e-4
  * Batch size: 32
- Evaluation:
  * Separate testing on File2 and File3
  * Per-class performance analysis
  * Training progress visualisation

### Detection Network

Pre-trained rchitectures:
- RetinaNet with ResNet50 backbone
- Faster R-CNN with ResNet50 backbone
- MobileNetV3-Large with FPN

Common features:
- 224x224 input resolution
- Pretrained ImageNet weights

Final model:
- RetinaNet with ResNet50 backbone (best performing)
- Training configuration:
  * Adam optimizer
  * Learning rate: 1e-4
  * Batch size: 4 (could not fit larger)
  * Early stopping at 99.0% IoU
  * Best model checkpointing
- Evaluation:
  * Separate testing on File4 and File5
  * IoU and coordinate-wise metrics
  * Training progress monitoring

## Performance

### Classification Results

TBD

### Detection Results

TBD